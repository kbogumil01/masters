test_full_frames: false  # Use chunks for testing (WITH VVC features!)

# Dataset paths - OPTIMIZED: chunks on fast WSL disk, decoded on USB
dataset:
  train:
    chunk_folder: "./chunks_dataset/chunks"           # FAST: internal disk
    orig_chunk_folder: "./chunks_dataset/orig_chunks"  # FAST: internal disk
    chunk_height: 132
    chunk_width: 132
  val:
    chunk_folder: "./chunks_dataset/chunks"           # FAST: internal disk
    orig_chunk_folder: "./chunks_dataset/orig_chunks"  # FAST: internal disk
    chunk_height: 132
    chunk_width: 132
  test:
    chunk_folder: "./chunks_dataset/chunks"           # FAST: internal disk
    orig_chunk_folder: "./chunks_dataset/orig_chunks"  # FAST: internal disk
    chunk_height: 132
    chunk_width: 132

# VVC features - OK on USB (cached in RAM, read once per video)
fused_maps_dir: "/mnt/d/data_mgr/decoded"

# Dataloader config - STABLE OPTIMIZED (max stable performance)
dataloader:
  batch_size: 16        # Balans: 2x więcej niż przed crash, bezpieczniej niż 24
  val_batch_size: 12    # Bezpieczne dla sanity check
  test_batch_size: 12
  n_step: 800           # ↑ z 450 - rzadsze walidacje = szybszy trening
  val_n_step: 3
  test_n_step: 5

# Training config - FAST TRAINING (< 10h target)
trainer:
  mode: gan
  separation_epochs: 5   # ↓ z 10 - szybszy start GAN
  channels_grad_scales:
    - 0.66666
    - 0.66666
    - 0.66666
  gan:
    epochs: 50            # ↓ z 200 - 50 epok często wystarcza!
    discriminator_lr: 0.001
    enhancer_lr: 0.00002
    discriminator_scheduler_gamma: 0.5
    discriminator_scheduler_milestones: [1, 2, 5, 10, 25, 50]  # Dopasowane do 50 epok
    enhancer_scheduler_gamma: 0.5
    enhancer_scheduler_milestones: [12, 25, 37, 50]  # Dopasowane do 50 epok
    saved_chunk_folder: enhanced/gan

# Discriminator - ULTRA-LIGHTWEIGHT for fast training (< 10h)
discriminator:
  save_to: 'experiments/gan/conv_dis.pth'
  features:
    kernel_size: 5  # ↓ z 7 - mniejszy kernel
    padding: 2
    stride: 2
    features: 32
    pool: false
  structure:
    blocks:
      - kernel_size: 4
        padding: 1
        stride: 2
        features: 64
      - kernel_size: 4
        padding: 1
        stride: 2
        features: 128
      - kernel_size: 4
        padding: 1
        stride: 2
        features: 256
      # USUNIĘTE: bloki 512 i 1024 (za ciężkie!)
  classifier:
    kernel_size: 8  # Zwiększony bo mamy tylko 3 bloki
    padding: 0
    stride: 1
    features: 1

# Enhancer (DenseNet) - COMPACT for speed
enhancer:
  implementation: dense
  save_to: 'experiments/gan/dense_enh.pth'
  # load_from: 'experiments/enhancer/dense.pth'  # Odkomentuj jeśli masz pretrained
  bn_size: 1.5
  features:
    kernel_size: 7  # ↓ z 9 - mniejszy
    padding: 3
    stride: 1
    features: 48    # ↓ z 64 - mniej features
    pool: false
    dense: true
  structure:
    blocks:
      - kernel_size: 5  # ↓ z 7
        padding: 2
        stride: 1
        features: 48    # ↓ z 64
        transition:
          mode: same
      - kernel_size: 3  # ↓ z 5
        padding: 1
        stride: 1
        features: 64    # ↓ z 96
        transition:
          mode: same
      - kernel_size: 3
        padding: 1
        stride: 1
        features: 48    # ↓ z 64
        transition:
          mode: same
      # USUNIĘTE: 2 ostatnie bloki (48 i 32 features)
  output_block:
    kernel_size: 3
    padding: 1
    stride: 1
    features: 3
    tanh: false
